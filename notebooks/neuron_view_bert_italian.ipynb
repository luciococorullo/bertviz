{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906c0fff",
   "metadata": {},
   "source": [
    "# BertViz Neuron View - Modello BERT Italiano\n",
    "\n",
    "Questo notebook dimostra come utilizzare BertViz per visualizzare le attenzioni di un modello BERT italiano.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Prima di tutto, assicuriamoci che tutte le dipendenze siano installate e configurate correttamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5727f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import delle librerie necessarie\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Aggiungiamo la directory del progetto al path\n",
    "project_dir = os.path.dirname(os.path.abspath(''))\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.insert(0, project_dir)\n",
    "\n",
    "print(f\"Project directory: {project_dir}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9865dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import delle librerie principali\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "# Import del nostro modello personalizzato\n",
    "from modeling_bert_italian import BertModelIT\n",
    "\n",
    "print(\"✓ Tutte le librerie importate con successo!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA disponibile: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc8aee3",
   "metadata": {},
   "source": [
    "## Caricamento del Modello BERT Italiano\n",
    "\n",
    "Useremo il modello `dbmdz/bert-base-italian-xxl-cased`, uno dei migliori modelli BERT per la lingua italiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15343be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome del modello BERT italiano pre-addestrato\n",
    "model_name = \"dbmdz/bert-base-italian-xxl-cased\"\n",
    "\n",
    "print(f\"Caricamento del modello: {model_name}\")\n",
    "print(\"Questo potrebbe richiedere alcuni minuti al primo avvio...\")\n",
    "\n",
    "# Carica il tokenizzatore standard\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Carica il nostro modello personalizzato con i pesi pre-addestrati\n",
    "# IMPORTANTE: impostiamo output_attentions=True per ottenere le attenzioni\n",
    "model = BertModelIT.from_pretrained(\n",
    "    model_name, \n",
    "    output_attentions=True\n",
    ")\n",
    "\n",
    "# Impostiamo il modello in modalità evaluation\n",
    "model.eval()\n",
    "\n",
    "print(\"✓ Modello e tokenizzatore caricati con successo!\")\n",
    "print(f\"Numero di parametri: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2262fe95",
   "metadata": {},
   "source": [
    "## Test del Modello con Testo Italiano\n",
    "\n",
    "Ora testiamo il modello con alcune frasi in italiano per verificare che funzioni correttamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf34f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frasi di test in italiano\n",
    "sentence_a = \"Il gatto si è seduto sul tappeto.\"\n",
    "sentence_b = \"Il cane dormiva sulla poltrona.\"\n",
    "\n",
    "print(\"Frasi di test:\")\n",
    "print(f\"  Frase A: {sentence_a}\")\n",
    "print(f\"  Frase B: {sentence_b}\")\n",
    "\n",
    "# Tokenizziamo le frasi\n",
    "inputs = tokenizer(\n",
    "    sentence_a, \n",
    "    sentence_b, \n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print(\"\\nTokens:\")\n",
    "print(f\"  Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"  Tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1204e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eseguiamo il forward pass per verificare che il modello funzioni\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(\"Output del modello:\")\n",
    "print(f\"  Last hidden state shape: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"  Numero di layer con attenzioni: {len(outputs.attentions)}\")\n",
    "\n",
    "if outputs.attentions:\n",
    "    attn = outputs.attentions[0]\n",
    "    print(f\"  Shape delle attenzioni (layer 0): {attn.shape}\")\n",
    "    print(f\"    (batch_size, num_heads, seq_length, seq_length)\")\n",
    "    \n",
    "print(\"\\n✓ Il modello funziona correttamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e169d",
   "metadata": {},
   "source": [
    "## Visualizzazione con BertViz Neuron View\n",
    "\n",
    "Ora utilizziamo BertViz per visualizzare le attenzioni del modello. La Neuron View mostra come i query e key vectors interagiscono per produrre i punteggi di attenzione.\n",
    "\n",
    "**Parametri:**\n",
    "- `layer`: il layer da visualizzare (0-11 per BERT base)\n",
    "- `head`: la testa di attenzione da visualizzare (0-11 per BERT base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb02dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione della Neuron View\n",
    "# Layer 2, Head 8 (puoi modificare questi valori)\n",
    "layer_to_view = 2\n",
    "head_to_view = 8\n",
    "\n",
    "print(f\"Visualizzazione del layer {layer_to_view}, head {head_to_view}\")\n",
    "print(\"Nota: La visualizzazione apparirà qui sotto\\n\")\n",
    "\n",
    "try:\n",
    "    show(\n",
    "        model, \n",
    "        \"bert\", \n",
    "        tokenizer, \n",
    "        sentence_a, \n",
    "        sentence_b, \n",
    "        layer=layer_to_view, \n",
    "        head=head_to_view\n",
    "    )\n",
    "    print(\"\\n✓ Visualizzazione completata!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Errore durante la visualizzazione: {e}\")\n",
    "    print(f\"Tipo di errore: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c4e379",
   "metadata": {},
   "source": [
    "## Esempi Aggiuntivi\n",
    "\n",
    "Proviamo con altre frasi italiane per esplorare diverse attenzioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f705a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio 2: Frasi con relazioni grammaticali più complesse\n",
    "sentence_a2 = \"La studentessa che studia matematica ha superato l'esame.\"\n",
    "sentence_b2 = \"Il professore le ha fatto i complimenti.\"\n",
    "\n",
    "print(f\"Frase A: {sentence_a2}\")\n",
    "print(f\"Frase B: {sentence_b2}\\n\")\n",
    "\n",
    "show(model, \"bert\", tokenizer, sentence_a2, sentence_b2, layer=5, head=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6757e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio 3: Frasi con termini tecnici\n",
    "sentence_a3 = \"L'intelligenza artificiale trasforma il modo in cui lavoriamo.\"\n",
    "sentence_b3 = \"Gli algoritmi di machine learning apprendono dai dati.\"\n",
    "\n",
    "print(f\"Frase A: {sentence_a3}\")\n",
    "print(f\"Frase B: {sentence_b3}\\n\")\n",
    "\n",
    "show(model, \"bert\", tokenizer, sentence_a3, sentence_b3, layer=8, head=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f93d1",
   "metadata": {},
   "source": [
    "## Esplorazione Interattiva\n",
    "\n",
    "Puoi modificare i seguenti parametri per esplorare diversi layer e head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53432844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri modificabili\n",
    "YOUR_SENTENCE_A = \"Scrivi qui la tua prima frase in italiano.\"\n",
    "YOUR_SENTENCE_B = \"Scrivi qui la tua seconda frase in italiano.\"\n",
    "YOUR_LAYER = 6  # 0-11 per BERT base\n",
    "YOUR_HEAD = 4   # 0-11 per BERT base\n",
    "\n",
    "print(f\"Visualizzazione personalizzata:\")\n",
    "print(f\"  Layer: {YOUR_LAYER}\")\n",
    "print(f\"  Head: {YOUR_HEAD}\\n\")\n",
    "\n",
    "show(\n",
    "    model, \n",
    "    \"bert\", \n",
    "    tokenizer, \n",
    "    YOUR_SENTENCE_A, \n",
    "    YOUR_SENTENCE_B, \n",
    "    layer=YOUR_LAYER, \n",
    "    head=YOUR_HEAD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeae2f0",
   "metadata": {},
   "source": [
    "## Informazioni sul Modello\n",
    "\n",
    "Alcune informazioni utili sul modello BERT italiano utilizzato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f72c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Informazioni sul modello:\")\n",
    "print(f\"  Nome: {model_name}\")\n",
    "print(f\"  Numero di layer: {model.config.num_hidden_layers}\")\n",
    "print(f\"  Numero di attention heads: {model.config.num_attention_heads}\")\n",
    "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"  Intermediate size: {model.config.intermediate_size}\")\n",
    "print(f\"  Max position embeddings: {model.config.max_position_embeddings}\")\n",
    "print(f\"  Vocabolario: {model.config.vocab_size} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee09f66",
   "metadata": {},
   "source": [
    "## Note Tecniche\n",
    "\n",
    "### Come Funziona\n",
    "\n",
    "1. **BertSelfAttentionIT**: Modifica il metodo `forward` per restituire `query_layer` e `key_layer` oltre all'output standard.\n",
    "\n",
    "2. **Propagazione**: Le modifiche si propagano attraverso `BertAttentionIT`, `BertLayerIT`, `BertEncoderIT` fino a `BertModelIT`.\n",
    "\n",
    "3. **Compatibilità**: Il modello è completamente compatibile con i pesi pre-addestrati di Hugging Face.\n",
    "\n",
    "### Parametri della Visualizzazione\n",
    "\n",
    "- **Layer**: Ogni layer del transformer apprende rappresentazioni diverse. I layer iniziali tendono a catturare caratteristiche sintattiche, mentre i layer finali catturano semantica più astratta.\n",
    "\n",
    "- **Head**: Ogni head di attenzione può specializzarsi in relazioni diverse (es. dipendenze sintattiche, co-riferimento, ecc.).\n",
    "\n",
    "### Risorse\n",
    "\n",
    "- [BertViz Documentation](https://github.com/jessevig/bertviz)\n",
    "- [BERT Paper](https://arxiv.org/abs/1810.04805)\n",
    "- [Modello BERT Italiano](https://huggingface.co/dbmdz/bert-base-italian-xxl-cased)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
